---
title: "Topic 2"
author: "Alexandra Yousefivand"
date: "4/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(jsonlite) #convert results from API queries into R-friendly formats 
library(tidyverse) 
library(tidytext) #text data management and analysis
library(ggplot2) #plot word frequencies and publication dates
```



```{r}
# create an object called x with the results of our query ("haaland" --> "katrina")
# the from JSON flatten the JSON object, then convert to a data frame
t <- fromJSON("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=katrina&api-key=NTKBHbsb6XFEkGymGumAiba7n3uBvs8V", flatten = TRUE) #the string following "key=" is your API key 

class(t) #what type of object is t?

t <- t %>% 
  data.frame()


# Inspect our data
class(t) # now what is it?
dim(t) # how big is it?
names(t) # what variables are we working with?
# t <- readRDS("nytDat.rds") #in case of API emergency :)
```


```{r eval = FALSE}
t$response.docs.snippet[9]

#assign a snippet to x to use as fodder for stringr functions.  You can follow along using the sentence on the next line.

x <- "The ruin of a region and the historic city of New Orleans could not be more important, and the tangle of destruction is nowhere near unwound." 

# tolower(x)
# str_split(x, ','); str_split(x, 't')
# str_replace(x, 'historic', 'without precedent')
# str_replace(x, ' ', '_') # first one
# str_replace_all(x, ' ', '_') # how do we replace all of them?
# 
# str_detect(x, 't'); str_detect(x, 'tive') ### is pattern in the string? T/F
# str_locate(x, 't'); str_locate_all(x, 'as')
```


```{r}
term <- "Katrina" # Need to use + to string together separate words
begin_date <- "20050823" # start of Hurricane Katrina
end_date <- "20050906" # two weeks later

#construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
                  "&begin_date=",begin_date,"&end_date=",end_date,
                  "&facet_filter=true&api-key=","NTKBHbsb6XFEkGymGumAiba7n3uBvs8V", sep="")

baseurl #examine our query url
```


```{r}
#this code allows for obtaining multiple pages of query results 
 initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) 

pages <- list()
for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(6) 
}

```

```{r}
nytDat <- rbind_pages(pages)
write_csv(nytDat, "nytDat.csv")
```



********************if code worked, start HERE********************************
```{r}
nytDat <- read.csv("nytDat.csv") # obtained from 

nytDat %>% 
  group_by(response.docs.type_of_material) %>%
  summarize(count=n()) %>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip()
```

```{r}
nytDat %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>% # remove time component
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  filter(count >= 2) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") + coord_flip()
```



```{r}
paragraph <- names(nytDat)[6] #The 6th column, "response.doc.lead_paragraph", is the one we want here.  
tokenized <- nytDat %>%
  unnest_tokens(word, paragraph) # convert from text to tidy text format
                                 # change from paragraph to a collection of single words

tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 5) %>% #illegible with all the words displayed
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```


```{r}
data(stop_words)

tokenized <- tokenized %>%
  anti_join(stop_words)

tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```


```{r}
#inspect the list of tokens (words)
# tokenized$word

clean_tokens <- str_replace_all(tokenized$word,"hurricane*","hurricane") %>%
                str_remove_all("[:digit:]") %>%  #remove all numbers
                str_remove_all("day$")      %>%  #remove days of the week
                str_remove_all("â€™s")        %>%  #remove Katrina's
                str_remove_all("'s")        %>%  #remove Katrina's different font
                str_remove_all("aug")       %>%  #remove month august
                str_remove_all("sept")      %>%  #remove month september
                str_remove_all("yester")    %>%  #remove yesterday
                str_remove_all("wednes")         #remove wednesday

tokenized$clean <- clean_tokens

tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 15) %>% #illegible with all the words displayed
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)
```


```{r}
#remove the empty strings
tib <- tokenized %>% 
         subset(clean!= "") %>% 
         subset(clean!=".") %>% 
         subset(clean!="a")

#reassign
tokenized <- tib

#try again
tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 15) %>% #illegible with all the words displayed
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)
```


